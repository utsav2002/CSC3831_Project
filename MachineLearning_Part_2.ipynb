{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuJhI64Qg-lp"
      },
      "source": [
        "# Starter point for the Machine Learning element of the coursework\n",
        "\n",
        "This part of the coursework uses the same dataset as used in the first part of the coursework. In this part of the coursework you are going to develop three machine learning models for predicting the **'median_house_value'** from the dataset. The focus here is on comparing the different models and looking at how you can improve them. You do not need to use a cleaned up version of the data (i.e. perform outlier removal in advance) but you may if you wish although please comment on this.\n",
        "\n",
        "Everything needed to complete this assignment should be available in the course slides, but external information from the interenet may prove useful and is encouraged. Please provide citations for resources used in the course."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `IMPORT PACKAGES`"
      ],
      "metadata": {
        "id": "j0aHYuP0wSlB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install matplotlib\n",
        "!pip install scikit-learn"
      ],
      "metadata": {
        "id": "RA9YHSkYwRHF",
        "outputId": "9880b5a0-6830-4f25-9979-42e3e2201b52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VA6pBhl5epn3"
      },
      "source": [
        "### Load libraries and read in data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KatzrPHcDdJT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "6462c9de-871a-4707-ae30-2b4d16da207c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 20640 entries, 0 to 20639\n",
            "Data columns (total 9 columns):\n",
            " #   Column              Non-Null Count  Dtype  \n",
            "---  ------              --------------  -----  \n",
            " 0   median_house_value  20640 non-null  float64\n",
            " 1   median_income       20640 non-null  float64\n",
            " 2   housing_median_age  20640 non-null  float64\n",
            " 3   total_rooms         20640 non-null  float64\n",
            " 4   total_bedrooms      20640 non-null  float64\n",
            " 5   population          20640 non-null  float64\n",
            " 6   households          20640 non-null  float64\n",
            " 7   latitude            20640 non-null  float64\n",
            " 8   longitude           20640 non-null  float64\n",
            "dtypes: float64(9)\n",
            "memory usage: 1.4 MB\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "houses = pd.read_csv('https://raw.githubusercontent.com/PaoloMissier/CSC3831-2021-22/main/IMPUTATION/TARGET-DATASETS/ORIGINAL/houses.csv', header=0)\n",
        "\n",
        "houses.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lb1pl1pjeoSN"
      },
      "source": [
        "## Data Preparation\n",
        "\n",
        "\n",
        "In this section you will perform feature selection, feature normalisation, and provide a rationale for your actions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6PaslWyackc"
      },
      "source": [
        "### Feature Selection\n",
        "Think about which features may be useful in predicting **'median_house_value'**, are all features in the provided data set useful? Is a subset all that's needed? What techniques can you utilise to make this determination?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQv6cEOsBBNk"
      },
      "source": [
        "The features population, total rooms, median income, and households are considered valuable indicators for predicting the median house value. Population reflects the demand for housing, while total rooms captures the size of the property. Median income represents the purchasing power of individuals, and households provide insights into housing supply and demand dynamics. Together, these features include economic and spatial aspects which can be strongly impact the median house value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5LkQ5cOackd"
      },
      "source": [
        "### Feature Normalisation\n",
        "Think about what normalisation/standardisation methods you should apply to the dataset given what you understand about the raw data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXDziJj_9G9a"
      },
      "source": [
        "I've select four features: <br>\n",
        "1) Rooms per Household - Houses with more rooms usually mean the house is larger. Generally, larger houses tend to have higher values, and by calculating this, we can capture the housing density or the availability of space, which can be indicative of the housing market value. <br>\n",
        "2) People per Household - This can reflect the population density or the occupation level of housing in different areas, and can also be an indicator of housing affordability, more number of people per house indicates for requirement for larger houses which can affect the median house value quite directly. <br>\n",
        "3) Occupancy per Room - This feature further examines the occupancy level of each house, can give insights about overcrowding/underutilization of space. Higher occupancy per room indicates the demand for larger houses or the scarcity of housing might be higher, impacting the median house value. <br>\n",
        "4) Income per household - Higher-income households generally have a higher purchasing power and can afford more expensive houses. This indicates the affordability aspect and its influence on the median house value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "phZIra1Nackc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "8a5bd75b-5628-4dad-b33c-0f565c4796b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   rooms_per_household  people_per_household  occupancy_per_room  \\\n",
            "0             6.984127              2.555556            0.365909   \n",
            "1             6.238137              2.109842            0.338217   \n",
            "2             8.288136              2.802260            0.338105   \n",
            "3             5.817352              2.547945            0.437991   \n",
            "4             6.281853              2.181467            0.347265   \n",
            "\n",
            "   income_per_household  \n",
            "0              0.066073  \n",
            "1              0.007295  \n",
            "2              0.041002  \n",
            "3              0.025768  \n",
            "4              0.014850  \n"
          ]
        }
      ],
      "source": [
        "# Calculate normalized features\n",
        "\n",
        "houses_normalised = houses.copy()\n",
        "\n",
        "houses_normalised['rooms_per_household'] = houses['total_rooms'] / houses['households']\n",
        "houses_normalised['people_per_household'] = houses['population'] / houses['households']\n",
        "houses_normalised['occupancy_per_room'] = houses['population'] / houses['total_rooms']\n",
        "houses_normalised['income_per_household'] = houses['median_income'] / houses['households']\n",
        "\n",
        "\n",
        "# Display the new features\n",
        "print(houses_normalised[['rooms_per_household', 'people_per_household','occupancy_per_room', 'income_per_household']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_M3F3pQgackd"
      },
      "source": [
        "### Rationale\n",
        "Provide your rationale for both Feature Selection and Feature Normalisation here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbrC_9Q5ackd"
      },
      "source": [
        "Feature selection is important to identify the most relevant features, simplify the model, and reduce overfitting. Feature normalization ensures that features are on a similar scale, improves algorithm performance, and enhances interpretability of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhYQd16ze4ci"
      },
      "source": [
        "## Train, Validate, Test Split\n",
        "In this section you will perform a train, validate, test split utilisation the knowledge learned in class and provide a rationale for your actions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlaPJsp3k0N8"
      },
      "source": [
        "### Functions of Calculate Mean-Sqaured Error, Mean-Absolute Error and R-Squared Error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "w6kKoyS4l_RK"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import r2_score\n",
        "import numpy as np\n",
        "\n",
        "def calc_mse(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    return mse\n",
        "\n",
        "def calc_mae(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    return mae\n",
        "\n",
        "def calc_r2(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    return r2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkJ3Qh5eonLK"
      },
      "source": [
        "### Function to Print Evaluation of the performance of Models and Returns MSE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "T5BsI6TGo6RJ"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, model_name, X_test, y_test):\n",
        "    mse = calc_mse(model, X_test, y_test)\n",
        "    mae = calc_mae(model, X_test, y_test)\n",
        "    r2 = calc_r2(model, X_test, y_test)\n",
        "\n",
        "    print(f'{model_name} Performance:')\n",
        "    print(\"Mean Squared Error (MSE):\", mse)\n",
        "    print(\"Mean Absolute Error (MAE):\", mae)\n",
        "    print(\"R-squared:\", r2)\n",
        "\n",
        "    return mse, mae, r2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6_sPCYaqAZB"
      },
      "source": [
        "### Function to Compare the three scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "X_hQ72N6qJXF"
      },
      "outputs": [],
      "source": [
        "def compare_mse(mse1, mse2):\n",
        "\n",
        "    diff_mse = 100 * (mse2 - mse1) / mse1\n",
        "\n",
        "    if diff_mse >= 0:\n",
        "        print('Increase in Mean Squared Error by {:0.2f}%'.format(np.abs(diff_mse)))\n",
        "    else:\n",
        "        print('Decrease in Mean Squared Error by {:0.2f}%'.format(np.abs(diff_mse)))\n",
        "\n",
        "def compare_mae(mae1, mae2):\n",
        "\n",
        "    diff_mae = 100 * (mae2 - mae1) / mae1\n",
        "\n",
        "    if diff_mae >= 0:\n",
        "        print('Increase in Mean Absolute Error by {:0.2f}%'.format(np.abs(diff_mae)))\n",
        "    else:\n",
        "        print('Decrease in Mean Absolute Error by {:0.2f}%'.format(np.abs(diff_mae)))\n",
        "\n",
        "def compare_r2(r2_1, r2_2):\n",
        "\n",
        "    diff_r2 = 100 * (r2_2 - r2_1) / r2_1\n",
        "\n",
        "    if diff_r2 >= 0:\n",
        "        print('Increase in R-Sqaured by {:0.2f}%'.format(np.abs(diff_r2)))\n",
        "    else:\n",
        "        print('Decrease in R-Sqaured by {:0.2f}%'.format(np.abs(diff_r2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jh4vHbNgrYNy"
      },
      "source": [
        "### Function to Compare The Different Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "hG2-xm96rfb4"
      },
      "outputs": [],
      "source": [
        "def compare_models(model1, model2, model_name1, model_name2, X_test, y_test):\n",
        "    mse_model_1 = calc_mse(model1, X_test, y_test)\n",
        "    mse_model_2 = calc_mse(model2, X_test, y_test)\n",
        "\n",
        "    mae_model_1 = calc_mae(model1, X_test, y_test)\n",
        "    mae_model_2 = calc_mae(model2, X_test, y_test)\n",
        "\n",
        "    r2_model_1 = calc_r2(model1, X_test, y_test)\n",
        "    r2_model_2 = calc_r2(model2, X_test, y_test)\n",
        "\n",
        "    diff_mse = 100 * (mse_model_2 - mse_model_1) / mse_model_1\n",
        "    diff_mae = 100 * (mae_model_2 - mae_model_1) / mae_model_1\n",
        "    diff_r2 = 100 * (r2_model_2 - r2_model_1) / r2_model_1\n",
        "\n",
        "    print('{} gives {:0.2f}% better Mean Squared Error Score than {}'.format(model_name2, diff_mse, model_name1))\n",
        "    print('{} gives {:0.2f}% better Mean Absolute Error Score than {}'.format(model_name2, diff_mae, model_name1))\n",
        "    print('{} gives {:0.2f}% better R-Sqaured than {}\\n'.format(model_name2, diff_r2, model_name1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXR4K_p7acke"
      },
      "source": [
        "### Perform Data Split\n",
        "Utilise the information from class or online to split your data into train, validate, and test partitions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LHqwFVZKFhI3"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Scaling\n",
        "scaler = StandardScaler()\n",
        "houses_scaled = scaler.fit_transform(houses)\n",
        "houses_scaled = pd.DataFrame(houses_scaled, columns=houses.columns)\n",
        "\n",
        "# Spliting Data (80% Training and 20% Testing)\n",
        "X = houses_scaled.drop(columns=['median_house_value'])\n",
        "y = houses_scaled['median_house_value']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# Further split the train set into train and validation sets (80% train, 20% validation)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opv46-kZacke"
      },
      "source": [
        "### Rationale\n",
        "Provide a rationale for how you made your train, test, split decision."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Zh3nEedacke"
      },
      "source": [
        "I chose an 80-20 split for the train and test sets, which is a commonly used ratio in machine learning projects. With 80% of the data allocated for training, the model can learn from a substantial amount of examples, while the remaining 20% serves as a reliable test set to evaluate its performance on unseen data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_u3ivZ2dfCtm"
      },
      "source": [
        "## Metric Selection\n",
        "In this section your will make appropriate metric selection for analysing your models and provide a rationale."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m298BNp3acke"
      },
      "source": [
        "### Select Appropriate Metrics\n",
        "Think about the models you are building, and what the appropriate metrics and scoring should be for those models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IwjmXi-acke"
      },
      "source": [
        "For this tasks, I will use common evaluation metrics including mean squared error (MSE), mean absolute error (MAE), and R-squared (coefficient of determination)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWSBe15Gacke"
      },
      "source": [
        "### Rationale\n",
        "Provide a rationale for your metrics and scoring selection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_p0LKo22acke"
      },
      "source": [
        "I selected evaluation metrics (MSE, MAE, R-squared) to assess the regression model's accuracy, error magnitude, and goodness-of-fit. These metrics provide insights into the model's predictive capabilities, aiding informed decisions for predicting house values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVN7d-rdfK8Q"
      },
      "source": [
        "## Machine Learning Model 1\n",
        "In this section your will select an appropriate machine learning model for predicting **'median_house_value'**, apply it to the dataset to perform this prediction on the test set created in the Train, Validate, Test Split section, and comment on the predictive ability of the model you selected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDxiDWM0acke"
      },
      "source": [
        "### Select and Build a Machine Learning Model 1\n",
        "Think about the task at hand, and select an appropriate model to build on the train and validate data. Try different sets of hyper-parameters to improve your model. <br>\n",
        "\n",
        "### RANDOM FOREST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OrCts1npGGXB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "44d45413-6a9f-489a-8ad3-9dfc8ea3986e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Base Model Performance:\n",
            "Mean Squared Error (MSE): 0.2245331280732427\n",
            "Mean Absolute Error (MAE): 0.31589171910306724\n",
            "R-squared: 0.7718443943618413\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Random Forest Modelling\n",
        "rf_model = RandomForestRegressor(n_estimators=100, max_depth=10)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluating the Base Model\n",
        "base_rf_model = evaluate(rf_model, 'Random Forest Base Model', X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdVJGbTwV315"
      },
      "source": [
        "### First Set of Hyper-Parameter: Randomised Search CV\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YRUZIUYiWKdA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "025697b5-8de9-43a7-9c3c-5f0b14261c00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Base Model Performance:\n",
            "Mean Squared Error (MSE): 0.19469252247612864\n",
            "Mean Absolute Error (MAE): 0.2857740037189359\n",
            "R-squared: 0.8021664296937416\n",
            "\n",
            "\n",
            "Random Forest Best Search Model Performance:\n",
            "Mean Squared Error (MSE): 0.22561546069471877\n",
            "Mean Absolute Error (MAE): 0.31593790140489986\n",
            "R-squared: 0.7707446000603329\n",
            "\n",
            "\n",
            "Increase in Mean Squared Error by 15.88%\n",
            "Increase in Mean Absolute Error by 10.56%\n",
            "Decrease in R-Sqaured by 3.92%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from math import sqrt\n",
        "\n",
        "# Base Random Forest model\n",
        "base_model_cv_rf = RandomForestRegressor(random_state=42)\n",
        "base_model_cv_rf.fit(X_train, y_train)\n",
        "\n",
        "# Hyperparameter grid for Randomized Search CV\n",
        "param_grid_cv_rf = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 5, 10],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['auto', 'sqrt']\n",
        "}\n",
        "\n",
        "# Randomized Search CV\n",
        "random_search_cv_rf = RandomizedSearchCV(\n",
        "    estimator=RandomForestRegressor(random_state=42),\n",
        "    param_distributions=param_grid_cv_rf,\n",
        "    n_iter=4,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    cv=5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit Randomized Search CV to training data\n",
        "random_search_cv_rf.fit(X_train, y_train)\n",
        "\n",
        "# Best parameter Search\n",
        "best_search_cv_rf = random_search_cv_rf.best_estimator_\n",
        "\n",
        "# Model's performance\n",
        "base_rf_cv_mse, base_rf_cv_mae, base_rf_cv_r2 = evaluate(base_model_cv_rf, 'Random Forest Base Model', X_test, y_test)\n",
        "print(\"\\n\")\n",
        "best_rf_cv_mse, best_rf_cv_mae, best_rf_cv_r2 = evaluate(best_search_cv_rf, 'Random Forest Best Search Model', X_test, y_test)\n",
        "print(\"\\n\")\n",
        "compare_mse(base_rf_cv_mse, best_rf_cv_mse)\n",
        "compare_mae(base_rf_cv_mae, best_rf_cv_mae)\n",
        "compare_r2(base_rf_cv_r2, best_rf_cv_r2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1ARi4rwXt9X"
      },
      "source": [
        "### Second Set of Hyper-Parameter: Randomised Search Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "exbzsklfX93S",
        "outputId": "18a28823-b490-4e8b-b1a0-8fd436da40a2"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Base Model Performance:\n",
            "Mean Squared Error (MSE): 0.19604711128496904\n",
            "Mean Absolute Error (MAE): 0.286184944366063\n",
            "R-squared: 0.8007899867932052\n",
            "\n",
            "\n",
            "Random Forest Best Search Model Performance:\n",
            "Mean Squared Error (MSE): 0.18942617554365418\n",
            "Mean Absolute Error (MAE): 0.2851348753127427\n",
            "R-squared: 0.8075177405857693\n",
            "\n",
            "\n",
            "Decrease in Mean Squared Error by 3.38%\n",
            "Decrease in Mean Absolute Error by 0.37%\n",
            "Increase in R-Sqaured by 0.84%\n"
          ]
        }
      ],
      "source": [
        "# Base Random Forest model\n",
        "base_model_scv_rf = RandomForestRegressor()\n",
        "base_model_scv_rf.fit(X_train, y_train)\n",
        "\n",
        "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
        "max_features = ['log2', 'sqrt', None]\n",
        "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
        "max_depth.append(None)\n",
        "min_samples_split = [2, 5, 10]\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "bootstrap = [True, False]\n",
        "\n",
        "random_grid_rf = {\n",
        "    'n_estimators': n_estimators,\n",
        "    'max_features': max_features,\n",
        "    'max_depth': max_depth,\n",
        "    'min_samples_split': min_samples_split,\n",
        "    'min_samples_leaf': min_samples_leaf,\n",
        "    'bootstrap': bootstrap\n",
        "}\n",
        "\n",
        "# Cross-Validation\n",
        "scv_search_rf = RandomizedSearchCV(estimator=base_model_scv_rf,\n",
        "                                param_distributions=random_grid_rf,\n",
        "                                n_iter=30,\n",
        "                                cv=3,\n",
        "                                verbose=2,\n",
        "                                random_state=42,\n",
        "                                n_jobs=-1)\n",
        "\n",
        "# Fit Randomized Cross-Validation to training data\n",
        "scv_search_rf.fit(X_train, y_train)\n",
        "\n",
        "# Best parameter Search\n",
        "best_search_scv_rf = scv_search_rf.best_estimator_\n",
        "\n",
        "# Model's performance\n",
        "base_rf_scv_mse, base_rf_scv_mae, base_rf_scv_r2 = evaluate(base_model_scv_rf, 'Random Forest Base Model', X_test, y_test)\n",
        "print(\"\\n\")\n",
        "best_rf_scv_mse, best_rf_scv_mae, best_rf_scv_r2 = evaluate(best_search_scv_rf, 'Random Forest Best Search Model', X_test, y_test)\n",
        "print(\"\\n\")\n",
        "compare_mse(base_rf_scv_mse, best_rf_scv_mse)\n",
        "compare_mae(base_rf_scv_mae, best_rf_scv_mae)\n",
        "compare_r2(base_rf_scv_r2, best_rf_scv_r2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DccLHBtacke"
      },
      "source": [
        "### Use Model 1 to Predict on Test Data\n",
        "Use the model you've trained to predict **'median_house_value'** on the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "TfDqVk1eacke",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "ec28729f-b248-4d87-e978-d0ba1d1d51e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Actual  Predicted\n",
            "20046 -1.379252  -1.315465\n",
            "3024  -1.395718  -1.143379\n",
            "15663  2.540411   2.322297\n",
            "20484  0.101776   0.526281\n",
            "9814   0.616539   0.447497\n",
            "         Actual  Predicted\n",
            "20046 -1.379252  -1.347455\n",
            "3024  -1.395718  -1.009125\n",
            "15663  2.540411   2.303932\n",
            "20484  0.101776   0.492772\n",
            "9814   0.616539   0.418237\n"
          ]
        }
      ],
      "source": [
        "# Predict 'median_house_value' using the trained model on the test data\n",
        "y_pred_best_model_cv_rf = best_search_cv_rf.predict(X_test)\n",
        "comparison_df_best_model_cv_rf = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred_best_model_cv_rf})\n",
        "\n",
        "# First few instances for comparison\n",
        "print(comparison_df_best_model_cv_rf.head())\n",
        "\n",
        "# Predict 'median_house_value' using the trained model on the test data\n",
        "y_pred_best_model_scv_rf = best_search_scv_rf.predict(X_test)\n",
        "comparison_df_best_model_scv_rf = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred_best_model_scv_rf})\n",
        "\n",
        "# First few instances for comparison\n",
        "print(comparison_df_best_model_scv_rf.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_QEC0niacke"
      },
      "source": [
        "### Rationale\n",
        "Random Forest is a suitable choice for this task because it can predict median house\n",
        "values very well as it can handle both numerical and categorical features, and handle outlier very well(which we saw right above), it also provides quite reliable estimates. However, its not been very helpful here but usually, it does play an impactful role in model training and testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STg_0uSiacke"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyCGVpjMackf"
      },
      "source": [
        "### Comment on Predictive Ability\n",
        "Think about the metrics and scoring received from the training and testing components. Think about the generalisability and quality of your results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEcuptjVackf"
      },
      "source": [
        "For the predictive ability of the optimized RF Model, it can be seen that a there is a strong relation between the predicted and the actual median house values. The metrics derived from both training and testing phases showcase that the model is adept in identifying complex relationships within the dataset. A good R^2 value suggests that major proportion of variance in house values have been accounted for by the model, therefore giving reliable predictions.\n",
        "\n",
        "The model's generalizability is shown by the findings, which demonstrate that it has discovered underlying patterns in the training data that apply to the unknown test set. This feature indicates that the model is well-tuned and balances fitting the training data with retaining flexibility to adjust to new data. Because of the model's performance on the test data, one can be confident on applying this model in real-world situations where it can provide insightful information and help to attain the housing values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEudJDaSackf"
      },
      "source": [
        "## Machine Learning Model 2\n",
        "In this section your will select an appropriate machine learning model for predicting **'median_house_value'**, apply it to the dataset to perform this prediction on the test set created in the Train, Validate, Test Split section, and comment on the predictive ability of the model you selected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaWrXVyCackf"
      },
      "source": [
        "### Select and Build a Machine Learning Model 2\n",
        "Think about the task at hand, and select an appropriate model to build on the train and validate data. Try different sets of hyper-parameters to improve your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JD9vaL7ackf"
      },
      "outputs": [],
      "source": [
        "# Support Vector Model\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "# Creating and training the model\n",
        "svr = SVR()\n",
        "svr.fit(X_train, y_train)\n",
        "base_model_svr = evaluate(svr, 'Support Vector base Model', X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5m0QvtksPgzP"
      },
      "source": [
        "### First Set of Hyper-Parameter: Randomised Search CV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "MFLy44Q_Pgd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "a15fb86d-677a-4447-ff47-3d2b6ef084e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Support Vector with Randomized Search CV Base Model Performance:\n",
            "Mean Squared Error (MSE): 0.25150612259191424\n",
            "Mean Absolute Error (MAE): 0.33433607829257045\n",
            "R-squared: 0.7444362343584996\n",
            "\n",
            "\n",
            "Support Vector Best Grid Search Model Performance:\n",
            "Mean Squared Error (MSE): 0.22882370035404007\n",
            "Mean Absolute Error (MAE): 0.31366066919910673\n",
            "R-squared: 0.7674846006616426\n",
            "\n",
            "\n",
            "Decrease in Mean Squared Error by 9.02%\n",
            "Decrease in Mean Absolute Error by 6.18%\n",
            "Increase in R-Sqaured by 3.10%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from scipy.stats import uniform, reciprocal\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Pipeline with preprocessing and SVR model\n",
        "pipeline_svr = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('svr', SVR())\n",
        "])\n",
        "\n",
        "# Parameter grid for RandomizedSearchCV\n",
        "param_grid_svr = {\n",
        "    'svr__kernel': ['linear', 'rbf'],\n",
        "    'svr__C': reciprocal(1, 100),\n",
        "    'svr__gamma': reciprocal(0.01, 1)\n",
        "}\n",
        "\n",
        "# Create the RandomizedSearchCV object\n",
        "random_search_svr = RandomizedSearchCV(estimator=pipeline_svr,\n",
        "                                       param_distributions=param_grid_svr,\n",
        "                                       n_iter=4,\n",
        "                                       scoring='neg_mean_squared_error',\n",
        "                                       cv=3,\n",
        "                                       random_state=42)\n",
        "\n",
        "# Fit the RandomizedSearchCV object to the data\n",
        "random_search_svr.fit(X_train, y_train)\n",
        "\n",
        "# Best parameter Search\n",
        "best_search_cv_svr = random_search_svr.best_estimator_\n",
        "\n",
        "# Model's performance\n",
        "base_svr_cv_mse, base_svr_cv_mae, base_svr_cv_r2 = evaluate(svr, 'Support Vector with Randomized Search CV Base Model', X_test, y_test)\n",
        "print(\"\\n\")\n",
        "best_svr_cv_mse, best_svr_cv_mae, best_svr_cv_r2 = evaluate(best_search_cv_svr, 'Support Vector Best Grid Search Model', X_test, y_test)\n",
        "print(\"\\n\")\n",
        "compare_mse(base_svr_cv_mse, best_svr_cv_mse)\n",
        "compare_mae(base_svr_cv_mae, best_svr_cv_mae)\n",
        "compare_r2(base_svr_cv_r2, best_svr_cv_r2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vL1p7UeyQlR3"
      },
      "source": [
        "### Second Set of Hyper-Parameter: Randomised Search Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8f6WDC4cQrGm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "d23968b2-d470-4248-b73c-82402c5c01e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
            "Support Vector Base Model Performance:\n",
            "Mean Squared Error (MSE): 0.25150612259191424\n",
            "Mean Absolute Error (MAE): 0.33433607829257045\n",
            "R-squared: 0.7444362343584996\n",
            "\n",
            "\n",
            "Support Vector Best Search Model Performance:\n",
            "Mean Squared Error (MSE): 0.2504465264264307\n",
            "Mean Absolute Error (MAE): 0.33720693308217126\n",
            "R-squared: 0.7455129253882034\n",
            "\n",
            "\n",
            "Decrease in Mean Squared Error by 0.42%\n",
            "Increase in Mean Absolute Error by 0.86%\n",
            "Increase in R-Sqaured by 0.14%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "base_model_svr = SVR()\n",
        "\n",
        "# Defining Parameter Grid for Support Vector\n",
        "kernel = ['linear', 'rbf', 'sigmoid']\n",
        "gamma = ['scale', 'auto']\n",
        "epsilon = np.arange(start=0.1, stop=0.6, step=0.1)\n",
        "shrinking = [True, False]\n",
        "cache_size = [int(x) for x in np.linspace(start=200, stop=1000, num=9)]\n",
        "\n",
        "random_grid_svr = {\n",
        "    'kernel': kernel,\n",
        "    'gamma': gamma,\n",
        "    'epsilon': epsilon,\n",
        "    'shrinking': shrinking,\n",
        "    'cache_size': cache_size\n",
        "}\n",
        "\n",
        "\n",
        "# Cross-Validation\n",
        "svr_search_scv_svr = RandomizedSearchCV(estimator=base_model_svr,\n",
        "                                param_distributions=random_grid_svr,\n",
        "                                n_iter=30,\n",
        "                                cv=3,\n",
        "                                verbose=2,\n",
        "                                random_state=42,\n",
        "                                n_jobs=-1)\n",
        "\n",
        "# Fit Randomized Cross-Validation to training data\n",
        "svr_search_scv_svr.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# Best parameter Search\n",
        "best_search_svr_scv = svr_search_scv_svr.best_estimator_\n",
        "\n",
        "# Model's performance\n",
        "base_svr_scv_mse, base_svr_scv_mae, base_svr_scv_r2 = evaluate(svr, 'Support Vector Base Model', X_test, y_test)\n",
        "print(\"\\n\")\n",
        "best_svr_scv_mse, best_svr_scv_mae, best_svr_scv_r2 = evaluate(best_search_svr_scv, 'Support Vector Best Search Model', X_test, y_test)\n",
        "print(\"\\n\")\n",
        "compare_mse(base_svr_scv_mse, best_svr_scv_mse)\n",
        "compare_mae(base_svr_scv_mae, best_svr_scv_mae)\n",
        "compare_r2(base_svr_scv_r2, best_svr_scv_r2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uv69buUoackf"
      },
      "source": [
        "### Use Model 2 to Predict on Test Data\n",
        "Use the model you've trained to predict **'median_house_value'** on the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "zoW3Syixackf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "844db69c-ed2e-4b11-da3a-f0fe10fa15e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Actual  Predicted\n",
            "20046 -1.379252  -1.245011\n",
            "3024  -1.395718  -0.393106\n",
            "15663  2.540411   2.297324\n",
            "20484  0.101776   0.305287\n",
            "9814   0.616539   0.528013\n",
            "         Actual  Predicted\n",
            "20046 -1.379252  -1.370268\n",
            "3024  -1.395718  -0.616170\n",
            "15663  2.540411   3.041298\n",
            "20484  0.101776   0.346181\n",
            "9814   0.616539   0.718314\n"
          ]
        }
      ],
      "source": [
        "# Predict 'median_house_value' using the trained model on the test data\n",
        "y_pred_best_model_cv_svr = best_search_svr_scv.predict(X_test)\n",
        "comparison_df_best_model_cv_svr = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred_best_model_cv_svr})\n",
        "\n",
        "# First few instances for comparison\n",
        "print(comparison_df_best_model_cv_svr.head())\n",
        "\n",
        "# Predict 'median_house_value' using the trained model on the test data\n",
        "y_pred_best_model_scv_svr = best_search_cv_svr.predict(X_test)\n",
        "comparison_df_best_model_scv_svr = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred_best_model_scv_svr})\n",
        "\n",
        "# First few instances for comparison\n",
        "print(comparison_df_best_model_scv_svr.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prQ3OU_packf"
      },
      "source": [
        "### Rationale\n",
        "Provide a rationale for model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fow4fmz1ackf"
      },
      "source": [
        "SVR is a very relevant choice for predicting median house values as it can handle nonlinear relationships, outliers, and high-dimensional features very effectively. SVR helps to minimize the error and provide accurate predictions for the data and requirement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEEoyRuAackf"
      },
      "source": [
        "### Comment on Predictive Ability\n",
        "Think about the metrics and scoring received from the training and testing components. Think about the generalisability and quality of your results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s94LzNf3ackl"
      },
      "source": [
        "The SVM model's relatively high R-squared score shows that it can display good proportion of variance in the housing value. However, the MSE and MAE suggest prediction accuracy can be improved. The hyperparameter tuning sessions via RandomizedSearchCV resulted in a notable decrease in MSE and MAE and an improvement in the R-squared value, showing the benefits of model optimization.\n",
        "\n",
        "The dataset looks to suit the SVM model, which performs in higher-dimensional areas. Given the slight gains after intensive hyperparameter adjustment, the default settings may have solved this issue near-optimally. This shows the SVM's predictive performance may be limited by feature representation or model complexity.\n",
        "\n",
        "The SVM's home value forecasts are accurate for broad estimates but may not be accurate for more complex ones. The SVM model is generalizable, but it needs more validation across datasets, maybe stretching regions or housing market situations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzxbDFJZackm"
      },
      "source": [
        "## Machine Learning Model 3\n",
        "In this section your will select an appropriate machine learning model for predicting **'median_house_value'**, apply it to the dataset to perform this prediction on the test set created in the Train, Validate, Test Split section, and comment on the predictive ability of the model you selected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEuPl_3Fackm"
      },
      "source": [
        "### Select and Build a Machine Learning Model 3\n",
        "Think about the task at hand, and select an appropriate model to build on the train and validate data. Try different sets of hyper-parameters to improve your model.\n",
        "\n",
        "### eXtreme Gradient Boosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "dDYTtlTEackm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "87f09c5f-8bb8-4592-ec22-6c7de9736e05"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
              "             colsample_bylevel=None, colsample_bynode=None,\n",
              "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
              "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "             gamma=None, grow_policy=None, importance_type=None,\n",
              "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
              "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
              "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "             multi_strategy=None, n_estimators=None, n_jobs=None,\n",
              "             num_parallel_tree=None, random_state=None, ...)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
              "             colsample_bylevel=None, colsample_bynode=None,\n",
              "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
              "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "             gamma=None, grow_policy=None, importance_type=None,\n",
              "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
              "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
              "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "             multi_strategy=None, n_estimators=None, n_jobs=None,\n",
              "             num_parallel_tree=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
              "             colsample_bylevel=None, colsample_bynode=None,\n",
              "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
              "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "             gamma=None, grow_policy=None, importance_type=None,\n",
              "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
              "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
              "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "             multi_strategy=None, n_estimators=None, n_jobs=None,\n",
              "             num_parallel_tree=None, random_state=None, ...)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# Gradient Boosting (XGBoost)\n",
        "import xgboost as xgb\n",
        "\n",
        "# Creating and training the XGBoost model\n",
        "xgbr = xgb.XGBRegressor()\n",
        "xgbr.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iU6XAPSBUVHD"
      },
      "source": [
        "### First Set of Hyper-Parameter: Randomised Search CV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "_hA754jzUa_8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "cc6fbde4-b217-43e4-f5ab-80b6a104bcac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost Base Model Performance:\n",
            "Mean Squared Error (MSE): 0.1764602416667473\n",
            "Mean Absolute Error (MAE): 0.2802490563802504\n",
            "R-squared: 0.8206928587598012\n",
            "\n",
            "\n",
            "XGBoost Best Search Model Performance:\n",
            "Mean Squared Error (MSE): 0.192223789122506\n",
            "Mean Absolute Error (MAE): 0.3010158368473712\n",
            "R-squared: 0.8046749920528389\n",
            "\n",
            "\n",
            "Increase in Mean Squared Error by 8.93%\n",
            "Increase in Mean Absolute Error by 7.41%\n",
            "Decrease in R-Sqaured by 1.95%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import uniform, randint\n",
        "\n",
        "# Defining the parameter grid for RandomizedSearchCV\n",
        "param_grid_xgb_cv = {\n",
        "    'n_estimators': randint(100, 1000),\n",
        "    'learning_rate': uniform(0.01, 0.5),\n",
        "    'max_depth': randint(3, 10),\n",
        "    'subsample': uniform(0.6, 0.4),\n",
        "    'colsample_bytree': uniform(0.6, 0.4),\n",
        "    'gamma': uniform(0, 5)\n",
        "}\n",
        "\n",
        "# Creating the RandomizedSearchCV object\n",
        "random_search_xgb_cv = RandomizedSearchCV(estimator=xgbr,\n",
        "                                       param_distributions=param_grid_xgb_cv,\n",
        "                                       n_iter=4,\n",
        "                                       scoring='neg_mean_squared_error',\n",
        "                                       cv=5,\n",
        "                                       random_state=42,\n",
        "                                       n_jobs=-1)\n",
        "\n",
        "\n",
        "# Fitting the RandomizedSearchCV object to the data\n",
        "random_search_xgb_cv.fit(X_train, y_train)\n",
        "\n",
        "# Getting the best model\n",
        "best_search_xgb_cv = random_search_xgb_cv.best_estimator_\n",
        "\n",
        "# Model's performance\n",
        "base_xgb_cv_mse, base_xgb_cv_mae, base_xgb_cv_r2 = evaluate(xgbr, 'XGBoost Base Model', X_test, y_test)\n",
        "print(\"\\n\")\n",
        "best_xgb_cv_mse, best_xgb_cv_mae, best_xgb_cv_r2 = evaluate(best_search_xgb_cv, 'XGBoost Best Search Model', X_test, y_test)\n",
        "print(\"\\n\")\n",
        "\n",
        "compare_mse(base_xgb_cv_mse, best_xgb_cv_mse)\n",
        "compare_mae(base_xgb_cv_mae, best_xgb_cv_mae)\n",
        "compare_r2(base_xgb_cv_r2, best_xgb_cv_r2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufOif74gUbSE"
      },
      "source": [
        "### Second Set of Hyper-Parameter: Randomised Search Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "WsE7sgSUUbs2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "99e0f38b-281a-4d4f-96e8-a855ca10b627"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
            "XGBoost Base Model Performance:\n",
            "Mean Squared Error (MSE): 0.1764602416667473\n",
            "Mean Absolute Error (MAE): 0.2802490563802504\n",
            "R-squared: 0.8206928587598012\n",
            "\n",
            "\n",
            "XGBoost Best Model Performance:\n",
            "Mean Squared Error (MSE): 0.16163999083328628\n",
            "Mean Absolute Error (MAE): 0.26951718654014584\n",
            "R-squared: 0.8357522102846003\n",
            "\n",
            "\n",
            "Decrease in Mean Squared Error by 8.40%\n",
            "Decrease in Mean Absolute Error by 3.83%\n",
            "Increase in R-Sqaured by 1.83%\n"
          ]
        }
      ],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import uniform, randint\n",
        "\n",
        "# Base model for XGBoost\n",
        "base_model_xgb = xgb.XGBRegressor(objective='reg:squarederror')\n",
        "base_model_xgb.fit(X_train, y_train)\n",
        "\n",
        "# Defining Parameter Grid for XGBoost\n",
        "param_grid_xgb_scv = {\n",
        "    'n_estimators': randint(100, 1000),\n",
        "    'max_depth': randint(3, 10),\n",
        "    'learning_rate': uniform(0.01, 0.59),\n",
        "    'subsample': uniform(0.5, 0.4),\n",
        "    'colsample_bytree': uniform(0.5, 0.4),\n",
        "    'gamma': uniform(0, 5)\n",
        "}\n",
        "\n",
        "# Randomized Search Cross-Validation\n",
        "xgb_search_scv = RandomizedSearchCV(estimator=base_model_xgb,\n",
        "                                param_distributions=param_grid_xgb_scv,\n",
        "                                n_iter=30,\n",
        "                                cv=3,\n",
        "                                verbose=2,\n",
        "                                random_state=42,\n",
        "                                n_jobs=-1)\n",
        "\n",
        "# Fit Randomized Search Cross-Validation to training data\n",
        "xgb_search_scv.fit(X_train, y_train)\n",
        "\n",
        "# Best parameter Search\n",
        "best_search_xgb_scv = xgb_search_scv.best_estimator_\n",
        "\n",
        "# Model's Performace\n",
        "base_xgb_scv_mse, base_xgb_scv_mae, base_xgb_scv_r2 = evaluate(base_model_xgb, 'XGBoost Base Model', X_test, y_test)\n",
        "print(\"\\n\")\n",
        "best_xgb_scv_mse, best_xgb_scv_mae, best_xgb_scv_r2 = evaluate(best_search_xgb_scv, 'XGBoost Best Model', X_test, y_test)\n",
        "print(\"\\n\")\n",
        "\n",
        "compare_mse(base_xgb_scv_mse, best_xgb_scv_mse)\n",
        "compare_mae(base_xgb_scv_mae, best_xgb_scv_mae)\n",
        "compare_r2(base_xgb_scv_r2, best_xgb_scv_r2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xL_CL_Iackm"
      },
      "source": [
        "### Use Model 3 to Predict on Test Data\n",
        "Use the model you've trained to predict **'median_house_value'** on the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "py6_eDXiackm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "ff60b79d-b3e4-48bf-ebe7-6e5056194692"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Actual  Predicted\n",
            "20046 -1.379252  -1.480393\n",
            "3024  -1.395718  -0.667132\n",
            "15663  2.540411   2.707046\n",
            "20484  0.101776   0.584431\n",
            "9814   0.616539   0.100060\n",
            "         Actual  Predicted\n",
            "20046 -1.379252  -1.385794\n",
            "3024  -1.395718  -1.013201\n",
            "15663  2.540411   3.151627\n",
            "20484  0.101776   0.473945\n",
            "9814   0.616539   0.478283\n"
          ]
        }
      ],
      "source": [
        "# Predict 'median_house_value' using the trained model on the test data\n",
        "y_pred_best_model_cv_xgb = best_search_xgb_cv.predict(X_test)\n",
        "comparison_df_best_model_cv_xgb = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred_best_model_cv_xgb})\n",
        "\n",
        "# First few instances for comparison\n",
        "print(comparison_df_best_model_cv_xgb.head())\n",
        "\n",
        "# Predict 'median_house_value' using the trained model on the test data\n",
        "y_pred_best_model_scv_xgb = best_search_xgb_scv.predict(X_test)\n",
        "comparison_df_best_model_scv_xgb = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred_best_model_scv_xgb})\n",
        "\n",
        "# First few instances for comparison\n",
        "print(comparison_df_best_model_scv_xgb.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5RddZqvackm"
      },
      "source": [
        "### Rationale\n",
        "Provide a rationale for model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSd9ykCeackm"
      },
      "source": [
        "XGBoost is also a well-suited model for predicting median house values as it excels in capturing complex interactions between features, handles missing values pretty well, and it provides high predictive accuracy, as evident above. It can also handle large scale dataset much easily than others as it took less time to execute itself fully."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ko4o0B_qackm"
      },
      "source": [
        "### Comment on Predictive Ability\n",
        "Think about the metrics and scoring received from the training and testing components. Think about the generalisability and quality of your results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDVlJ5Dkackm"
      },
      "source": [
        "The XGBoost model's high R-squared value shows its ability to forecast house value variation. With the lowest MSE and MAE among evaluated models, the model predicts accurately and reliably.\n",
        "\n",
        "The first set of hyperparameter tuning indicated a small decrease in predictive performance, showing the original values were effective. The second tuning session reduced MSE and MAE and increased R-squared. This improvement indicated the effect of tuning XGBoost's hyperparameter and possibility for more optimisation.\n",
        "\n",
        "The XGBoost approach seems generalizable across datasets. However, the performance variation before and after hyperparameter adjustment shows the need for extensive validation to prevent overfitting and ensuring consistency. After adjusting, the model's metrics increased, suggesting that XGBoost can accurately predict house values with higher accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJNNqoHXackm"
      },
      "source": [
        "## Comparison Between Models\n",
        "In this section you will comment on the difference in predictive ability between models, the difference in analysis metrics between models, and the pros/cons of each model as you understand it."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare best Random Forest model with the best SVR model\n",
        "compare_models(best_search_scv_rf, best_search_cv_svr, 'Best Random Forest', 'Best SVR', X_test, y_test)\n",
        "\n",
        "# Compare best SVR model with the best XGBoost model\n",
        "compare_models(best_search_cv_svr, best_search_xgb_cv, 'Best SVR', 'Best XGBoost', X_test, y_test)\n",
        "\n",
        "# Compare best XGBoost model with the best Random Forest model\n",
        "compare_models(best_search_xgb_cv, best_search_scv_rf, 'Best XGBoost', 'Best Random Forest', X_test, y_test)\n",
        "\n",
        "# You can also compare the base models with their respective best models\n",
        "# Compare base Random Forest with its best model\n",
        "compare_models(rf_model, best_search_scv_rf, 'Base Random Forest', 'Best Random Forest', X_test, y_test)\n",
        "\n",
        "# Compare base SVR with its best model\n",
        "compare_models(svr, best_search_cv_svr, 'Base SVR', 'Best SVR', X_test, y_test)\n",
        "\n",
        "# Compare base XGBoost with its best model\n",
        "compare_models(xgbr, best_search_xgb_cv, 'Base XGBoost', 'Best XGBoost', X_test, y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "UxosbFYAwnXC",
        "outputId": "33fe9814-d653-46df-c1a3-6350ddd3fbc7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best SVR gives 20.80% better Mean Squared Error Score than Best Random Forest\n",
            "Best SVR gives 10.00% better Mean Absolute Error Score than Best Random Forest\n",
            "Best SVR gives -4.96% better R-Sqaured than Best Random Forest\n",
            "\n",
            "Best XGBoost gives -15.99% better Mean Squared Error Score than Best SVR\n",
            "Best XGBoost gives -4.03% better Mean Absolute Error Score than Best SVR\n",
            "Best XGBoost gives 4.85% better R-Sqaured than Best SVR\n",
            "\n",
            "Best Random Forest gives -1.46% better Mean Squared Error Score than Best XGBoost\n",
            "Best Random Forest gives -5.28% better Mean Absolute Error Score than Best XGBoost\n",
            "Best Random Forest gives 0.35% better R-Sqaured than Best XGBoost\n",
            "\n",
            "Best Random Forest gives -15.64% better Mean Squared Error Score than Base Random Forest\n",
            "Best Random Forest gives -9.74% better Mean Absolute Error Score than Base Random Forest\n",
            "Best Random Forest gives 4.62% better R-Sqaured than Base Random Forest\n",
            "\n",
            "Best SVR gives -9.02% better Mean Squared Error Score than Base SVR\n",
            "Best SVR gives -6.18% better Mean Absolute Error Score than Base SVR\n",
            "Best SVR gives 3.10% better R-Sqaured than Base SVR\n",
            "\n",
            "Best XGBoost gives 8.93% better Mean Squared Error Score than Base XGBoost\n",
            "Best XGBoost gives 7.41% better Mean Absolute Error Score than Base XGBoost\n",
            "Best XGBoost gives -1.95% better R-Sqaured than Base XGBoost\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISIVRzq6ackm"
      },
      "source": [
        "### Difference in Predictive Ability\n",
        "Think about what you understand if the predictive ability from each model based on scores gained, comment on the difference between each model and suggest why you believe this to be the case. Additionally comment on the difference in hyper-parameter selection between your models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUMXFmqiackm"
      },
      "source": [
        "I find variations in each model's prediction abilities due to algorithmic complexity and hyperparameter adjustment. For eg. the Random Forest model prevents overfitting by averaging many decision trees, offering good baseline prediction ability. Tuning the hyperparameters using Randomised Search CV improved performance, demonstrating that the model's original setup was not ideal and that it improved after going through a wider parameter space.\n",
        "\n",
        "However, the Support Vector Machine model began with a lower R-squared value, suggesting a less precise data fit. I've read and I believe that the SVMs focus on margin maximisation and may not reflect the dataset's complexity without proper kernel and parameter selection. The SVM model enhanced predictive ability after tuning hyperparameters, showing the importance of kernel and regularisation strength selection for high-dimensional feature space.\n",
        "\n",
        "XGBoost, at the start itself beat the other two model because of its gradient boosting architecture(that successively constructs trees to rectify faults and optimise efficiency). Interestingly, hyperparameter adjustment did not improve performance, showing that the default values were effective. XGBoost's regularised boosting strategy may not need substantial modification to attain great predictive accuracy while being amonst the fastest.\n",
        "\n",
        "Hyperparameter selection differences amongst models show each algorithm's distinct qualities and sensitivities. Random Forest and SVM improved significantly with modification, suggesting their default settings were unsuitable for this dataset. In comparison, XGBoost's limited improvements post-tuning may indicate that its underlying algorithm is resistant to hyperparameter choice or because Randomised Search CV's search space does not contain the ideal set for actual performance improvements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HA8r2HkTackm"
      },
      "source": [
        "### Difference in Analysis Metrics\n",
        "Think about the analysis metrics you received from each model, comment on the use in the difference of these metrics and suggesting which model is \"better\" or \"worse\" when comparing them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGEA_jqWackm"
      },
      "source": [
        "The analytical metrics (MSE, MAE, R^2), each provides a distinct perspective for model evaluation. By squaring mistakes, MSE limits greater differences more severely, which is important for estimating house values. As an average of absolute mistakes, MAE is a simpler measure of average prediction error. R^2 measures the model's predictive ability by indicating the percentage of variation in the dependent variable that is predicted from the independent variables.\n",
        "\n",
        "XGBoost had the lowest MSE and MAE, indicating its predictions were more accurate. We want to predict the house values correctly, because even little modifications might have large effects. Though Random Forest Model was quite slow compared to XGBoost, it made reliable predictions that improved with hyperparameter modification. Despite being the worst of the three, the Support Vector Machine improved after tuning, suggesting it may do better under other circumstances or with a different parameter search space.\n",
        "\n",
        "Ultimately, the better model here is XGBoost, it is the one that minimizes error and maximizes explanatory power, aligning closely with the actual house values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjBbYn-wackm"
      },
      "source": [
        "### Pros/Cons\n",
        "Think about the models and analysis metrics you've gathered, and suggest the positives and negatives of each model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zt9su2abackm"
      },
      "source": [
        "When considering the pros and cons of each model in the context of predicting median house values, it's evident that each model has its strengths and challenges.\n",
        "\n",
        "The Random Forest is great because it's really good at handling complex relationships and non-linear patterns. This is important when it comes to predicting housing value because real estate markets can be really complicated. However, using this method can be computationally expensive and may lead to overfitting if not tuned correctly. This is supported by the slight increase in error that occurred after adjusting the hyperparameters.\n",
        "\n",
        "The Support Vector Machine is really good at finding the most effective boundary between different outputs. This is especially useful when the data is not clearly separated. The main drawback is that you have to be very careful when choosing the kernel function and hyperparameters to prevent overfitting. This is evident from the minimal improvement observed after tuning.\n",
        "\n",
        "XGBoost is known for its outstanding performance and speed, especially when dealing with large datasets. The results have shown the lowest errors, which suggests that our predictions are highly accurate. However, there is a risk of overfitting if the hyperparameters are not set correctly. Additionally, achieving optimal results with this method requires a considerable amount of parameter tuning, as demonstrated by the noticeable improvement observed after tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_cXzq8Oackn"
      },
      "source": [
        "## Conclusion\n",
        "In this section you will draw conclusions about the models you've selected, and efficacy of different analysis metrics you've used, and any additional comments or concerns about the models or metrics you've observed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tn448Izackn"
      },
      "source": [
        "### Model Selection\n",
        "Think about all of the information you've learned both theoretically from the slides and practically through completing this assingment about the models you've selected. What conclusions can you draw about the models, and what situations would you pick one model over another?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twd-DHQiackn"
      },
      "source": [
        "Based on both theoretical knowledge and practical application, it is clear that the choice of a model depends on the specific characteristics of the data and the task of prediction. The Random Forest model is known for its robustness and ability to handle outliers effectively, which makes it a great choice for datasets that contain many anomalies. Additionally, interpretability is highly advantageous as it allows for transparent explanations when making decisions. Vector Machines are known for their ability to capture complex relationships in datasets that are not extremely large. One of their key strengths is the kernel trick, which further enhances their capabilities. However, XGBoost is considered the top choice when working with large datasets that have complex patterns and interactions. The gradient boosting framework we use is highly effective in handling different types of data and distributions. It consistently provides excellent predictive accuracy, as shown by our lowest mean squared error (MSE) and highest R-squared (R^2) scores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUEj9Cj4ackn"
      },
      "source": [
        "### Analysis Metrics\n",
        "Think about the metrics you've selected and what you've learned both thetoretically from the slides and practically through this assignment. What conclusions can you draw about these metrics, and what situations might you pick one set of metrics over another? (Here you may comment on metrics which you have not selected in this assignment)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svsRisTLackn"
      },
      "source": [
        "The chosen metrics (MSE, MAE, and R^2) thoroughly evaluate the models. The Mean Squared Error (MSE) focuses on larger errors, making it ideal for circumstances when even minor errors are unacceptable. MAE measures average error size simply, making it easier to understand in real-world scenarios. The R^2 number reflects how well the model explains data variation. When outliers are not an issue or errors are distributed equally, we may utilise measures like Mean Bias Error (MBE) to establish error direction or the F1 score, accuracy, precision and recall for other tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48hZSst6ackn"
      },
      "source": [
        "### Any Additional Conclusions\n",
        "For those of you which have noticed things or made conclusions which do not neatly fit into any other section, please put them here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "az-2gvrSackn"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}